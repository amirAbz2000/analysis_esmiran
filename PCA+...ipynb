{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCs4-ydisizo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4eoOzSxmtcBy",
    "outputId": "51a05a5f-2f48-4340-8a7b-3052e1a29d70"
   },
   "outputs": [],
   "source": [
    "file_path = '/Users/zeynali/Desktop/Ø¯ÛŒØªØ§ Ø§Ø³Ù…ÛŒØ±Ø§Ù†/(experimental) Ù…Ø±Ø¯Ø§Ø¯/mordad.csv'\n",
    "print(\"Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Polars...\")\n",
    "df = pl.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgoXAqaXuPaZ",
    "outputId": "d9a8a5a2-dcda-41a5-a317-4e168a6c8302"
   },
   "outputs": [],
   "source": [
    "print(\"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯!\")\n",
    "print(\"Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "FZ5RpPeARO8i",
    "outputId": "358b9f5a-5e0f-4582-a826-410cacdece87"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ii1PvWj4uZ56"
   },
   "outputs": [],
   "source": [
    "time_column = 'datetime'  # Ù†Ø§Ù… Ø³ØªÙˆÙ† Ø²Ù…Ø§Ù†\n",
    "feature_columns = [col for col in df.columns if col != time_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select([col for col in feature_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_pl = df2.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text = 'Correlation Matrix for Features'\n",
    "feature_names = correlation_matrix_pl.columns\n",
    "z_values = correlation_matrix_pl.to_numpy()\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "        z=z_values,\n",
    "        x=feature_names,\n",
    "        y=feature_names,\n",
    "        colorscale='RdBu_r',\n",
    "        zmin=-1,\n",
    "        zmax=1,\n",
    "        colorbar=dict(title=\"Correlation\")))\n",
    "\n",
    "fig.update_layout(\n",
    "        title={'text': title_text, 'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top'},\n",
    "        height=735,\n",
    "        width= 735,\n",
    "        xaxis={'tickangle': 45},\n",
    "        yaxis={'tickangle': 0},)\n",
    "\n",
    "# fig.write_html(\"interactive_correlation_heatmap.html\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PjpkVKTub3H",
    "outputId": "fd4c920b-77c0-4783-b3aa-6d885fce6183"
   },
   "outputs": [],
   "source": [
    "print(\"\\nğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³Ù†Ø³ÙˆØ±Ù‡Ø§ÛŒ Ø®Ø§Ù…ÙˆØ´ (Ø¨Ø§ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ ØµÙØ±)...\")\n",
    "\n",
    "# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³ØªÙˆÙ†\n",
    "stds = df.select(pl.col(feature_columns).std())\n",
    "\n",
    "# Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø± Ø¢Ù†â€ŒÙ‡Ø§ ØµÙØ± Ø§Ø³Øª\n",
    "disabled_cols = []\n",
    "for col in feature_columns:\n",
    "    if stds.select(pl.col(col)).item() == 0:\n",
    "        disabled_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3KV0SztudBx",
    "outputId": "24da6268-ba6d-4cd0-e060-daac81b3579b"
   },
   "outputs": [],
   "source": [
    "print(disabled_cols)\n",
    "print(len(disabled_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disabled = pd.DataFrame(disabled_cols, columns=['Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡'])\n",
    "total_count_html = f\"<h3>ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡: {len(disabled_cols)}</h3>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_table_html = df_disabled.to_html(index=False, justify='center')\n",
    "final_html = total_count_html + cols_table_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"disabled_columns_report.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(final_html)\n",
    "# print(f\"âœ… ÙØ§ÛŒÙ„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ù…Ø³ÛŒØ±: '{file_name}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1leVQybueux",
    "outputId": "2377ba28-9be4-41d8-e3b5-3763d9608499"
   },
   "outputs": [],
   "source": [
    "df_clean = df.select([col for col in df.columns if col not in disabled_cols])\n",
    "print(f\"   ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡: {len(df_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_faetures = [col for col in df_clean.columns ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_faetures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_faetures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_columns = [col for col in df_clean.columns if col != time_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values_var = df_clean.select(count_columns).mean().row(0)\n",
    "std_values_var = df_clean.select(count_columns).std().row(0)\n",
    "\n",
    "# 5. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Z (Z-Score) Ùˆ Ø§Ù…ØªÛŒØ§Ø² Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ\n",
    "zscore_expressions = [\n",
    "    ((pl.col(col) - mean_values_var[i]) / pl.lit(std_values_var[i]))\n",
    "    for i, col in enumerate(count_columns)\n",
    "]\n",
    "\n",
    "df_clean = df_clean.with_columns(\n",
    "    pl.mean_horizontal(\n",
    "        [expr**2 for expr in zscore_expressions]\n",
    "    ).alias(\"anomaly_score\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = df_clean.select(\"anomaly_score\").to_series().mean() + 3 * df_clean.select(\"anomaly_score\").to_series().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly_zscore = df_clean.filter(pl.col(\"anomaly_score\") > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡:\", df_anomaly_zscore.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.filter(pl.col(\"anomaly_score\") <= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯!\")\n",
    "print(\"Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(\"anomaly_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in df_clean.columns if col != time_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean[feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IsolationForest(n_estimators=300, contamination=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['anomaly_label'] = model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_iso_tree = df_clean[df_clean['anomaly_label'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ Ø¨Ø§ Isolation Forest: {len(anomalies_iso_tree)}\")\n",
    "print(\"\\nÛµ Ù…ÙˆØ±Ø¯ Ø§ÙˆÙ„:\")\n",
    "print(anomalies_iso_tree.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['datetime'] = pd.to_datetime(df_clean['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pl.from_pandas(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.filter(pl.col(\"anomaly_label\") == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(\"anomaly_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯!\")\n",
    "print(\"Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡:\", anomalies_iso_tree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_iso_tree= pl.from_pandas(anomalies_iso_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(anomalies_iso_tree))\n",
    "print(type(df_anomaly_zscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_iso_tree.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_iso_tree = anomalies_iso_tree.drop(\"anomaly_label\")\n",
    "df_anomaly_zscore = df_anomaly_zscore.drop(\"anomaly_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡:\", df_anomaly_zscore.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pl.concat([anomalies_iso_tree, df_anomaly_zscore], how=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡:\", df_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_concat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡:\", df_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.with_columns(\n",
    "    pl.col(\"datetime\").str.to_datetime(\n",
    "        # Ø§ÛŒÙ† ÙØ±Ù…Øª Ø¨Ø±Ø§ÛŒ ØªØ¬Ø²ÛŒÙ‡ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø§ Ø¯Ù‚Øª Ù…ÛŒÚ©Ø±ÙˆØ«Ø§Ù†ÛŒÙ‡ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª\n",
    "        format=\"%Y-%m-%dT%H:%M:%S%.f\" \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡:\", df_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_days = df_final.select(\n",
    "    pl.col(\"datetime\").dt.date().alias(\"date\")\n",
    ").unique().sort(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_days.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_anomaly_counts = (\n",
    "    df_final.group_by(\n",
    "        # Ù…Ø±Ø­Ù„Ù‡ Û±: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÛŒØ® (Ø¨Ø¯ÙˆÙ† Ø²Ù…Ø§Ù†) Ùˆ Ø§ÛŒØ¬Ø§Ø¯ Ø³ØªÙˆÙ† 'date'\n",
    "        pl.col(\"datetime\").dt.date().alias(\"date\")\n",
    "    )\n",
    "    .len() # Ù…Ø±Ø­Ù„Ù‡ Û²: Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ Ø³Ø·Ø±Ù‡Ø§ Ø¯Ø± Ù‡Ø± Ú¯Ø±ÙˆÙ‡ (Ø³ØªÙˆÙ† Ø®Ø±ÙˆØ¬ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 'len' Ù†Ø§Ù… Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯)\n",
    "    .sort(\"len\", descending=True) # Ù…Ø±Ø­Ù„Ù‡ Û³: Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ DataFrame Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³ØªÙˆÙ† 'len'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_anomaly_counts.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_anomaly_counts_pd = daily_anomaly_counts.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jdatetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdatetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_shamsi(date_gregorian):\n",
    "    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² Ù†ÙˆØ¹ datetime.date ÛŒØ§ Timestamp Ø¨Ø§Ø´Ø¯\n",
    "    if isinstance(date_gregorian, pd.Timestamp):\n",
    "        date_gregorian = date_gregorian.date()\n",
    "        \n",
    "    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ jdatetime\n",
    "    date_shamsi = jdatetime.date.fromgregorian(\n",
    "        year=date_gregorian.year, \n",
    "        month=date_gregorian.month, \n",
    "        day=date_gregorian.day\n",
    "    )\n",
    "    # ÙØ±Ù…Øª Ø¯Ù‡ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø±Ø´ØªÙ‡ (Ù…Ø«Ù„Ø§Ù‹ 1403/05/10)\n",
    "    return date_shamsi.strftime(\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_anomaly_counts_pd['shamsi_date'] = daily_anomaly_counts_pd['date'].apply(convert_to_shamsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_anomaly_counts_pd.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    daily_anomaly_counts_pd,\n",
    "    x=\"shamsi_date\",             # *** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø±ÛŒØ® Ø´Ù…Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­ÙˆØ± X ***\n",
    "    y=\"len\",\n",
    "    title=\"ØªØ¹Ø¯Ø§Ø¯ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ (ØªØ§Ø±ÛŒØ® Ø´Ù…Ø³ÛŒ)\",\n",
    "    labels={\n",
    "        \"shamsi_date\": \"ØªØ§Ø±ÛŒØ® Ø´Ù…Ø³ÛŒ\", \n",
    "        \"len\": \"ØªØ¹Ø¯Ø§Ø¯ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ\"\n",
    "    },\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ù†Ù…Ø§ÛŒØ´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ\n",
    "fig.update_layout(\n",
    "    font_family=\"Arial\", # ØªÙ†Ø¸ÛŒÙ… ÙÙˆÙ†Øª Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ÙØ§Ø±Ø³ÛŒ\n",
    "    title_x=0.5, # Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù† Ø¹Ù†ÙˆØ§Ù† Ø¯Ø± ÙˆØ³Ø·\n",
    ")\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø­ÙˆØ± X Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¨Ù‡ØªØ± ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§\n",
    "fig.update_xaxes(\n",
    "    tickangle=-45, # Ú†Ø±Ø®Ø´ ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªØ¯Ø§Ø®Ù„\n",
    "    dtick=1 # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù†Ù…Ø§ÛŒØ´ Ù‡Ø± Ø±ÙˆØ² (Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ²Ù‡Ø§ Ø²ÛŒØ§Ø¯ Ø§Ø³ØªØŒ Plotly Ø®ÙˆØ¯Ø´ ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯)\n",
    ")\n",
    "\n",
    "# Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ÙØ§ÛŒÙ„ HTML\n",
    "output_html_file = \"daily_anomaly_bar_chart_shamsi.html\"\n",
    "# fig.write_html(output_html_file)\n",
    "\n",
    "print(f\"Ù†Ù…ÙˆØ¯Ø§Ø± Ø¨Ø§ ØªØ§Ø±ÛŒØ® Ø´Ù…Ø³ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ '{output_html_file}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_of_day = pl.col(\"datetime\").dt.hour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_with_segment = df_final.with_columns(\n",
    "    pl.when(hour_of_day.is_between(7, 13))\n",
    "        .then(pl.lit(\"Ø¸Ù‡Ø±\"))  # 10:00 ØªØ§ 17:59\n",
    "    .when(hour_of_day.is_between(14, 19))\n",
    "        .then(pl.lit(\"Ø¹ØµØ±\"))  # 18:00 ØªØ§ 21:59\n",
    "    .otherwise(pl.lit(\"Ø´Ø¨\"))  # 22:00 ØªØ§ 09:59\n",
    "    .alias(\"Time_Segment\")\n",
    ")\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ú†Ù†Ø¯ Ø±Ø¯ÛŒÙ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯\n",
    "print(\"DataFrame Ø¨Ø§ Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯ Time_Segment:\")\n",
    "print(df_final_with_segment[['datetime', 'Time_Segment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_with_segment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_counts_by_segment = df_final_with_segment.group_by(\"Time_Segment\").len()\n",
    "\n",
    "# Ø¨Ø±Ø§ÛŒ PlotlyØŒ Ø¨Ø§ÛŒØ¯ Polars DataFrame Ø±Ø§ Ø¨Ù‡ Pandas DataFrame ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ….\n",
    "anomaly_counts_pd = anomaly_counts_by_segment.to_pandas()\n",
    "\n",
    "print(\"Ø´Ù…Ø§Ø±Ø´ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø®Ø´ Ø²Ù…Ø§Ù†ÛŒ (Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±):\")\n",
    "print(anomaly_counts_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_pie = px.pie(\n",
    "    anomaly_counts_pd,\n",
    "    names='Time_Segment',  # Ø³ØªÙˆÙ†ÛŒ Ú©Ù‡ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ø´â€ŒÙ‡Ø§ Ø±Ø§ Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
    "    values='len',          # Ø³ØªÙˆÙ†ÛŒ Ú©Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ø±Ø´â€ŒÙ‡Ø§ Ø±Ø§ Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (ØªØ¹Ø¯Ø§Ø¯ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ)\n",
    "    title='ØªÙˆØ²ÛŒØ¹ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø®Ø´ Ø²Ù…Ø§Ù†ÛŒ (Ø¸Ù‡Ø±ØŒ Ø¹ØµØ±ØŒ Ø´Ø¨)',\n",
    "    # Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±ØµØ¯ Ø±ÙˆÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±\n",
    "    hole=.3, # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø§ÛŒØ±Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ø³ÙˆØ±Ø§Ø® (Donut Chart) Ø¨Ø±Ø§ÛŒ Ø¬Ø°Ø§Ø¨ÛŒØª Ø¨ÛŒØ´ØªØ± (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ù†Ù…Ø§ÛŒØ´ ÙØ§Ø±Ø³ÛŒ\n",
    "fig_pie.update_layout(\n",
    "    font_family=\"Arial\", # ØªÙ†Ø¸ÛŒÙ… ÙÙˆÙ†Øª Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ÙØ§Ø±Ø³ÛŒ\n",
    "    title_x=0.5,         # Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù† Ø¹Ù†ÙˆØ§Ù† Ø¯Ø± ÙˆØ³Ø·\n",
    "    legend_title=\"Ø¨Ø®Ø´ Ø²Ù…Ø§Ù†ÛŒ\"\n",
    ")\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±ØµØ¯ Ùˆ Ù…Ù‚Ø¯Ø§Ø±\n",
    "fig_pie.update_traces(\n",
    "    textposition='inside', # Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù† Ù…ØªÙ† Ø¯Ø§Ø®Ù„ Ø¨Ø±Ø´â€ŒÙ‡Ø§\n",
    "    textinfo='percent+value' # Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±ØµØ¯ Ùˆ Ù…Ù‚Ø¯Ø§Ø±\n",
    ")\n",
    "\n",
    "\n",
    "# --- Ú¯Ø§Ù… Û³: Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙØ§ÛŒÙ„ HTML ---\n",
    "output_html_file = \"anomaly_distribution_pie_chart.html\"\n",
    "# fig_pie.write_html(output_html_file)\n",
    "\n",
    "print(f\"Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø§ÛŒØ±Ù‡â€ŒØ§ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ '{output_html_file}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
    "fig_pie.show()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_with_segment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highly_skewed_features_updated(df: pl.DataFrame, threshold: float = 2.0) -> dict:\n",
    "    \"\"\"\n",
    "    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú†ÙˆÙ„Ú¯ÛŒ Ùˆ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¨Ø§ Ú†ÙˆÙ„Ú¯ÛŒ Ø¨Ø§Ù„Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² pl.selectors.numeric()\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ú¯Ø§Ù… A: Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§ Ø±ÙˆØ´ Ø¬Ø¯ÛŒØ¯ (Ø±ÙØ¹ DeprecationWarning)\n",
    "    # Ø§Ø² pl.selectors.numeric() Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø®ÙˆØ¯Ú©Ø§Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
    "    numeric_cols = df.select(pl.selectors.numeric()).columns\n",
    "    \n",
    "    if not numeric_cols:\n",
    "        return {}\n",
    "    \n",
    "    # Ú¯Ø§Ù… B: Ø§ÛŒØ¬Ø§Ø¯ Ø¹Ø¨Ø§Ø±Ø§Øª Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú†ÙˆÙ„Ú¯ÛŒ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)\n",
    "    # Ø§Ø² pl.selectors.numeric() Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø± select Ù†ÛŒØ² Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯.\n",
    "    \n",
    "    skew_results_df = df.select(\n",
    "        [pl.col(col).skew().alias(col) for col in numeric_cols]\n",
    "    )\n",
    "    \n",
    "    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØªØ§ÛŒØ¬ Ú†ÙˆÙ„Ú¯ÛŒ\n",
    "    skewness_values = skew_results_df.row(0, named=True)\n",
    "    \n",
    "    # Ú¯Ø§Ù… C: ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ÙÛŒÚ†Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ |Skewness| > threshold Ø§Ø³Øª\n",
    "    highly_skewed_features = {\n",
    "        name: skew\n",
    "        for name, skew in skewness_values.items()\n",
    "        if abs(skew) > threshold\n",
    "    }\n",
    "    \n",
    "    return highly_skewed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_skewed_map = get_highly_skewed_features_updated(df_clean, threshold=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### ğŸ” Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„ Ú†ÙˆÙ„Ú¯ÛŒ Ø¨Ø§ Polars ###\")\n",
    "print(f\"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ: {len([col for col in df_clean.columns if df_clean[col].dtype in pl.NUMERIC_DTYPES])}\")\n",
    "print(f\"ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¨Ø§ Ú†ÙˆÙ„Ú¯ÛŒ Ø¨Ø§Ù„Ø§ (Ø®Ø§Ø±Ø¬ Ø§Ø² [-2, 2]): {len(highly_skewed_map)}\")\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(\"## ğŸš¨ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ú†ÙˆÙ„Ú¯ÛŒ Ø¨Ø§Ù„Ø§ (Skewness > 2 ÛŒØ§ Skewness < -2):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if highly_skewed_map:\n",
    "    # Ø§ÛŒØ¬Ø§Ø¯ DataFrame Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ (Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´)\n",
    "    result_df = pl.DataFrame({\n",
    "        \"Feature\": list(highly_skewed_map.keys()),\n",
    "        \"Skewness\": list(highly_skewed_map.values())\n",
    "    }).with_columns(\n",
    "        # ÙØ±Ù…Øªâ€ŒØ¯Ù‡ÛŒ Ù…Ù‚Ø¯Ø§Ø± Skewness Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ø¨Ù‡ØªØ±\n",
    "        pl.col(\"Skewness\").map_elements(lambda x: f\"{x:.3f}\", return_dtype=pl.String)\n",
    "    )\n",
    "    \n",
    "    print(result_df)\n",
    "else:\n",
    "    print(\"Ù‡ÛŒÚ† ÙÛŒÚ†Ø±ÛŒ Ø¨Ø§ Ú†ÙˆÙ„Ú¯ÛŒ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² 2 ÛŒØ§ Ú©Ù…ØªØ± Ø§Ø² -2 ÛŒØ§ÙØª Ù†Ø´Ø¯.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def get_highly_kurtotic_features_updated(df: pl.DataFrame, threshold: float = 2.0) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø´ÛŒØ¯Ú¯ÛŒ (Kurtosis) Ùˆ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ø¨Ø§ Ú©Ø´ÛŒØ¯Ú¯ÛŒ Ø¨Ø§Ù„Ø§ (Leptokurtic)\n",
    "    Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² pl.selectors.numeric() Ø¯Ø± Polars.\n",
    "    \n",
    "    ØªÙˆØ¬Ù‡: Ú©Ø´ÛŒØ¯Ú¯ÛŒ Polars Ø¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ú©Ø´ÛŒØ¯Ú¯ÛŒ Ø§Ø¶Ø§ÙÛŒ (Excess Kurtosis) Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ú¯Ø§Ù… A: Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ\n",
    "    # Ø§Ø² pl.selectors.numeric() Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø®ÙˆØ¯Ú©Ø§Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
    "    numeric_cols = df.select(pl.selectors.numeric()).columns\n",
    "    \n",
    "    if not numeric_cols:\n",
    "        print(\"Ù‡Ø´Ø¯Ø§Ø±: Ù‡ÛŒÚ† Ø³ØªÙˆÙ† Ø¹Ø¯Ø¯ÛŒâ€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø´ÛŒØ¯Ú¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.\")\n",
    "        return {}\n",
    "    \n",
    "    # Ú¯Ø§Ù… B: Ø§ÛŒØ¬Ø§Ø¯ Ø¹Ø¨Ø§Ø±Ø§Øª Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø´ÛŒØ¯Ú¯ÛŒ (Kurtosis)\n",
    "    # Ø§Ø² ØªØ§Ø¨Ø¹ .kurtosis() Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø´ÛŒØ¯Ú¯ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
    "    kurtosis_results_df = df.select(\n",
    "        [pl.col(col).kurtosis().alias(col) for col in numeric_cols]\n",
    "    )\n",
    "    \n",
    "    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØªØ§ÛŒØ¬ Ú©Ø´ÛŒØ¯Ú¯ÛŒ\n",
    "    # row(0, named=True) Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¯Ø±Ù…ÛŒâ€ŒØ¢ÙˆØ±Ø¯.\n",
    "    kurtosis_values = kurtosis_results_df.row(0, named=True)\n",
    "    \n",
    "    # Ú¯Ø§Ù… C: ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ÙÛŒÚ†Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ |Kurtosis| > threshold Ø§Ø³Øª\n",
    "    # Ù…Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ø´ÛŒØ¯Ú¯ÛŒ Ù…Ø«Ø¨Øª (Leptokurtic) Ù‡Ø³ØªÛŒÙ… Ú©Ù‡ Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø¯Ù…â€ŒÙ‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ† Ø§Ø³Øª.\n",
    "    highly_kurtotic_features = {\n",
    "        name: kurtosis\n",
    "        for name, kurtosis in kurtosis_values.items()\n",
    "        # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…Ø§ Ù‡Ù… Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø«Ø¨Øª Ùˆ Ù‡Ù… Ù…Ù†ÙÛŒ Ø´Ø¯ÛŒØ¯ Ø±Ø§ ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…\n",
    "        if abs(kurtosis) > threshold\n",
    "    }\n",
    "    \n",
    "    return highly_kurtotic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kurtosis_threshold = 3.0 \n",
    "results = get_highly_kurtotic_features_updated(df_clean, threshold=kurtosis_threshold)\n",
    "\n",
    "print(f\"Ù†ØªÛŒØ¬Ù‡ Ú©Ø´ÛŒØ¯Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ø³ØªØ§Ù†Ù‡ |{kurtosis_threshold}|:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Ø§ÛŒØ¬Ø§Ø¯ DataFrame Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ (Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´)\n",
    "    kurtosis_df = pl.DataFrame({\n",
    "        \"Feature\": list(results.keys()),\n",
    "        \"kurtosis\": list(results.values())\n",
    "    })\n",
    "    \n",
    "    print(kurtosis_df)\n",
    "else:\n",
    "    print(\"Ù‡ÛŒÚ† ÙÛŒÚ†Ø±ÛŒ Ø¨Ø§ Ú†ÙˆÙ„Ú¯ÛŒ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² 2 ÛŒØ§ Ú©Ù…ØªØ± Ø§Ø² -2 ÛŒØ§ÙØª Ù†Ø´Ø¯.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kurtosis_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"highly_kurtosis_features.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kurtosis_df.write_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "# ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… df Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Polars Ø´Ù…Ø§Ø³Øª Ùˆ feature_name Ù‡Ù…Ø§Ù† 'f178' Ø§Ø³Øª\n",
    "feature_name = 'f60'\n",
    "# ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ† Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø¨Ù‡ Ù„ÛŒØ³Øª/Ø¢Ø±Ø§ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Plotly\n",
    "data_to_plot = df_clean.select(pl.col(feature_name)).to_series().to_list()\n",
    "\n",
    "def plot_interactive_histogram(data: list, feature: str, title: str):\n",
    "    \"\"\"Ø±Ø³Ù… Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… ØªØ¹Ø§Ù…Ù„ÛŒ Ø¨Ø§ Plotly.\"\"\"\n",
    "    fig = px.histogram(\n",
    "        x=data,\n",
    "        nbins=100,  # ØªØ¹Ø¯Ø§Ø¯ Ø³Ø·Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÙˆØ¶ÙˆØ­ Ø¨ÛŒØ´ØªØ±\n",
    "        title=title,\n",
    "        opacity=0.7,\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    fig.update_layout(xaxis_title=\"Ù…Ù‚Ø¯Ø§Ø± ÙÛŒÚ†Ø±\", yaxis_title=\"ÙØ±Ø§ÙˆØ§Ù†ÛŒ\")\n",
    "    # Show the plot in the notebook/environment\n",
    "    fig.show()\n",
    "\n",
    "# 1. Ø±Ø³Ù… Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ú©Ø§Ù…Ù„\n",
    "plot_interactive_histogram(\n",
    "    data_to_plot, \n",
    "    feature_name, \n",
    "    f'Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ú©Ø§Ù…Ù„ ØªØ¹Ø§Ù…Ù„ÛŒ ÙÛŒÚ†Ø±: {feature_name}'\n",
    ")\n",
    "\n",
    "# 2. Ø±Ø³Ù… Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø²ÙˆÙ… Ø´Ø¯Ù‡ (Ø¨Ø±Ø§ÛŒ ØªÙ…Ø±Ú©Ø² Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ)\n",
    "# Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ø§Ù„Ø§ÛŒ 99% Ø±Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø±Ú©Ø² Ø¯ÛŒØ¯Ù‡ Ø´ÙˆØ¯\n",
    "Q99 = np.percentile(data_to_plot, 99)\n",
    "data_zoomed = [x for x in data_to_plot if x <= Q99]\n",
    "\n",
    "plot_interactive_histogram(\n",
    "    data_zoomed, \n",
    "    feature_name, \n",
    "    f'Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… ØªØ¹Ø§Ù…Ù„ÛŒ ÙÛŒÚ†Ø±: {feature_name} (Ø²ÙˆÙ… Ø´Ø¯Ù‡ØŒ Ø²ÛŒØ± ØµØ¯Ú© Û¹Û¹)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² describe() ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ† Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±\n",
    "statistical_summary = df_clean.select(pl.col(feature_name)).describe()\n",
    "\n",
    "print(f\"Ø®Ù„Ø§ØµÙ‡â€ŒÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ ÙÛŒÚ†Ø±: {feature_name}\")\n",
    "print(statistical_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(selected_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness_df = df_clean.select([\n",
    "    pl.col(col).skew().alias(f\"skew_{col}\")\n",
    "    for col in selected_pca\n",
    "]).to_dict(as_series=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness_map = {\n",
    "    col: list(skewness_df[f\"skew_{col}\"]) [0]\n",
    "    for col in selected_pca\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKEWNESS_THRESHOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    col for col, skew_val in skewness_map.items()\n",
    "    if (skew_val is not None) and (abs(skew_val) > SKEWNESS_THRESHOLD)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯!\")\n",
    "print(\"Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pca = [\n",
    "    col for col in df_clean.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(selected_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness_df = df_clean.select([\n",
    "    pl.col(col).skew().alias(f\"skew_{col}\")\n",
    "    for col in selected_pca\n",
    "]).to_dict(as_series=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness_map = {\n",
    "    col: list(skewness_df[f\"skew_{col}\"]) [0]\n",
    "    for col in selected_pca\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_nan_inf = []\n",
    "\n",
    "for col, skew_val in skewness_map.items():\n",
    "    # Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø± Ø³Ù‡ Ø­Ø§Ù„Øª Ù†Ø§Ù…Ø¹ØªØ¨Ø±: NoneØŒ NaN ÛŒØ§ Inf\n",
    "    # Ø§ÛŒÙ† Ø³Ø§Ø¯Ù‡ ØªØ±ÛŒÙ† Ùˆ Ø§Ù…Ù† ØªØ±ÛŒÙ† ÙÛŒÙ„ØªØ± Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¢Ù…Ø§Ø±ÛŒ Ø§Ø³Øª.\n",
    "    is_nan_or_none = skew_val is None or (isinstance(skew_val, float) and math.isnan(skew_val))\n",
    "    is_inf = isinstance(skew_val, float) and math.isinf(skew_val)\n",
    "\n",
    "    if is_nan_or_none or is_inf:\n",
    "        columns_to_drop_nan_inf.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columns_to_drop_nan_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(columns_to_drop_nan_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯!\")\n",
    "print(\"Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙˆÙ„ÛŒÙ‡:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_clean.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30)) # Ù…Ø«Ø§Ù„: Ù…ÛŒ ØªÙˆØ§Ù†ÛŒØ¯ Ø§ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¯ Ø¨Ù‡ØªØ± ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯\n",
    "\n",
    "# 3. Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ø­Ø±Ø§Ø±ØªÛŒ (Heatmap)\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=False,  # Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ§Ø¯ ÙÛŒÚ†Ø±ØŒ Ù†Ù…Ø§ÛŒØ´ Ù…Ù‚Ø§Ø¯ÛŒØ± (annot=True) Ø¨Ø§Ø¹Ø« Ø´Ù„ÙˆØºÛŒ Ø²ÛŒØ§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
    "    cmap='coolwarm', # ÛŒÚ© color map Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒ\n",
    "    fmt=\".2f\", # ÙØ±Ù…Øª Ù†Ù…Ø§ÛŒØ´ Ø§Ø¹Ø¯Ø§Ø¯ (Ø§Ú¯Ø± annot=True Ø¨Ø§Ø´Ø¯)\n",
    "    linewidths=0.5, # Ø®Ø·ÙˆØ· Ø¨ÛŒÙ† Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§\n",
    "    linecolor='white',\n",
    "    cbar=True # Ù†Ù…Ø§ÛŒØ´ Ù†ÙˆØ§Ø± Ø±Ù†Ú¯\n",
    ")\n",
    "\n",
    "plt.title('Correlation Matrix for 194 Features', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_series = df_clean.select(\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = df_clean.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_sample_scaled = scaler.fit_transform(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"âœ… Shape: {X_sample_scaled.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø§Ø¬Ø±Ø§ÛŒ PCA\n",
    "# Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ n_components=None Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ ØªÙ…Ø§Ù… ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ….\n",
    "pca = PCA(n_components=None)\n",
    "pca.fit(X_sample_scaled)\n",
    "\n",
    "# Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ ØªØ¬Ù…Ø¹ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ (Cumulative Explained Variance)\n",
    "explained_variance_ratio_cumulative = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot Ø¨Ø±Ø§ÛŒ ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¤Ù„ÙÙ‡â€ŒÙ‡Ø§ \n",
    "# Ø§ÛŒÙ† Ù†Ù…ÙˆØ¯Ø§Ø± Ø¨Ù‡ Ø´Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø²Ø§Ù†ÙˆÛŒÛŒ (Elbow) Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ Ù…Ø¤Ù„ÙÙ‡â€ŒÙ‡Ø§Ø³Øª.\n",
    "\n",
    "# ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¤Ù„ÙÙ‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø«Ù„Ø§Ù‹ Û¹ÛµÙª ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ø±Ø§ Ø­ÙØ¸ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.\n",
    "n_components_90 = np.argmax(explained_variance_ratio_cumulative >= 0.99) + 1\n",
    "print(f\"âœ… ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¤Ù„ÙÙ‡â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ 90Ùª ÙˆØ§Ø±ÛŒØ§Ù†Ø³: {n_components_90}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø§Ø¯Ø§Ù…Ù‡ Ú©Ø¯ Python (ÙØ±Ø§Ù…ÙˆØ´ Ù†Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ matplotlib Ø±Ø§ import Ú©Ù†ÛŒØ¯)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ø§Ø¬Ø±Ø§ÛŒ PCA Ù…Ø¬Ø¯Ø¯ Ø¨Ø§ n_components=n_components_95\n",
    "final_pca = PCA(n_components=2)\n",
    "final_pca.fit(X_sample_scaled)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"ğŸ“Œ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ù…Ø¤Ù„ÙÙ‡ Ù‡Ø§:\")\n",
    "print(f\"PC1: {final_pca.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "print(f\"PC2: {final_pca.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "print(f\"ÙˆØ§Ø±ÛŒØ§Ù†Ø³ ØªØ¬Ù…Ø¹ÛŒ: {final_pca.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "\n",
    "# Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Scree Plot Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ ÙˆØ§Ø¶Ø­ØªØ±\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
    "plt.title('Scree Plot (Cumulative Explained Variance)')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_optimal = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_final = PCA(n_components=n_components_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_np = pca_final.fit_transform(X_sample_scaled) # ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ X_sample_scaled Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ù…Ø§Ø³Øª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_pca_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [f\"PC{i+1}\" for i in range(n_components_optimal)]\n",
    "pca_results_df = pl.DataFrame(\n",
    "    X_pca_np, \n",
    "    schema=column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ø´Ú©Ù„ (Shape) Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯Ù‡: {pca_results_df.shape}\")\n",
    "print(pca_results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pca_results_df.hstack(datetime_series)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
